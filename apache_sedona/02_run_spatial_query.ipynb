{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Apache Sedona Tutorial - Spatial Join\n",
    "\n",
    "This tutorial shows you how to create & analyse geospatial dataframes in Spark using Apache Sedona; as well as visualise the results\n",
    "\n",
    "---\n",
    "\n",
    "### Process\n",
    "1. Initialise a Spark session with Sedona enabled\n",
    "2. Load boundary & point dataframes from parquet files\n",
    "3. Convert them to geospatial dataframes\n",
    "4. Perform a spatial join\n",
    "    - Fix the performance\n",
    "5. Export the result to a Geopandas dataframe\n",
    "6. Map the points, coloured by boundary type\n",
    "\n",
    "---\n",
    "\n",
    "### Data Used\n",
    "\n",
    "- Boundary data is the ABS 2016 Census Remoteness Areas\n",
    "- Point data is a randomised set of points, based on ABS 2016 Census Meshblock centroids\n",
    "\n",
    "Â© Australian Bureau of Statistics (ABS), Commonwealth of Australia\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Import packages and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python packages\n",
    "\n",
    "import os\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set input path for parquet files\n",
    "input_path = os.path.join(os.getcwd(), \"data\")\n",
    "print(input_path)\n",
    "\n",
    "# set max number of processes (defaults to number of physical CPUs)\n",
    "num_processors = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"Spatial Join Tutorial\")\n",
    "         .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "         .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "         .config(\"spark.cores.max\", num_processors)\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "print(\"Spark {} session initialised (ignore any 'illegal reflective' errors - it's a minor Java 11 issue)\".format(spark.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Register Sedona's User Defined Types (UDTs) and Functions (UDFs) with the Spark session\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes\n",
    "\n",
    "#### 1. Load boundary data from gzipped parquet files\n",
    "\n",
    "Boundary geometries are polygons stored as OGC Well Known Text (WKT) strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bdy_wkt_df = spark.read.parquet(os.path.join(input_path, \"boundaries\"))\n",
    "bdy_wkt_df.printSchema()\n",
    "bdy_wkt_df.show(5)\n",
    "\n",
    "print(\"Loaded {} records\".format(bdy_wkt_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bdy number (last character of bdy ID) to bdy type - to enable display ordering in map\n",
    "bdy_wkt_df2 = bdy_wkt_df \\\n",
    "    .withColumn(\"bdy_type\", f.concat(f.substring(bdy_wkt_df[\"bdy_id\"], -1, 1), f.lit(\" - \"), bdy_wkt_df[\"bdy_type\"]))\n",
    "\n",
    "# show 5 rows ordered randomly\n",
    "bdy_wkt_df2.orderBy(f.rand()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a view of the DataFrame to enable SQL queries\n",
    "bdy_wkt_df2.createOrReplaceTempView(\"bdy_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load point data\n",
    "\n",
    "Spatial data is stored in latitude & longitude (double precision) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "point_wkt_df = spark.read.parquet(os.path.join(input_path, \"points\"))\n",
    "point_wkt_df.printSchema()\n",
    "point_wkt_df.show(5, False)\n",
    "\n",
    "print(\"Loaded {} records\".format(point_wkt_df.count()))\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_wkt_df.createOrReplaceTempView(\"point_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create geospatial dataframes\n",
    "\n",
    "#### 1. Create boundary geometries from WKT strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bdy_df = spark.sql(\"select bdy_id, bdy_type, state, ST_GeomFromWKT(wkt_geom) as geom from bdy_wkt\") \\\n",
    "    .repartition(96, \"state\")\n",
    "bdy_df.printSchema()\n",
    "bdy_df.show(5)\n",
    "\n",
    "print(\"{} partitions\".format(bdy_df.rdd.getNumPartitions()))\n",
    "\n",
    "# create view to enable SQL queries\n",
    "bdy_df.createOrReplaceTempView(\"bdy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create point geometries from lat/long fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "point_df = spark.sql(\"select point_id, state, ST_Point(longitude, latitude) as geom from point_wkt\") \\\n",
    "    .repartition(96, \"state\")\n",
    "point_df.printSchema()\n",
    "point_df.show(5, False)\n",
    "\n",
    "print(\"{} partitions\".format(point_df.rdd.getNumPartitions()))\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_df.createOrReplaceTempView(\"pnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a spatial join to boundary tag the points\n",
    "\n",
    "##### Note:\n",
    "1. One of the dataframes will be spatially indexed automatically to speed up the query\n",
    "2. It's an inner join; point records could be lost in coastal areas or where there are gaps in the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "sql = \"\"\"SELECT pnt.point_id,\n",
    "                bdy.bdy_id,\n",
    "                bdy.bdy_type,\n",
    "                bdy.state,\n",
    "                pnt.geom\n",
    "         FROM pnt\n",
    "         INNER JOIN bdy ON ST_Intersects(pnt.geom, bdy.geom)\"\"\"\n",
    "join_df = spark.sql(sql) \\\n",
    "    .cache()  # cache can save processing time when calling the same dataframe more than once\n",
    "join_df.printSchema()\n",
    "join_df.show(5, False)\n",
    "\n",
    "join_count = join_df.count()\n",
    "\n",
    "print(\"Boundary tagged {} points\".format(join_count))\n",
    "print(\"Query took {}\".format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export result to Geopandas for visualisation\n",
    "\n",
    "*Note: doesn't scale to big data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "\n",
    "# convert to Pandas dataframe first (ordered by bdy_type for better visualisation)\n",
    "pandas_df = join_df.orderBy(f.desc(\"bdy_type\")).toPandas()\n",
    "\n",
    "# then convert to Geopandas dataframe\n",
    "geopandas_df = geopandas.GeoDataFrame(pandas_df, geometry=\"geom\")\n",
    "\n",
    "geopandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(20, 20))\n",
    "\n",
    "# set background colour\n",
    "ax.set_facecolor('#EEEEEE')\n",
    "\n",
    "# create map of points by bdy type\n",
    "geopandas_df.plot(\n",
    "    column=\"bdy_type\",\n",
    "    legend=True,\n",
    "    cmap='YlOrRd',\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map NSW only\n",
    "fig2, ax2 = plt.subplots(1, figsize=(20, 20))\n",
    "ax2.set_facecolor('#EEEEEE')\n",
    "\n",
    "geopandas_df.loc[geopandas_df[\"state\"] == \"New South Wales\"].plot(\n",
    "    column=\"bdy_type\",\n",
    "    legend=True,\n",
    "    cmap='YlOrRd',\n",
    "    ax=ax2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}