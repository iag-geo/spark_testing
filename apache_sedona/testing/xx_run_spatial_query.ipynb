{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Geospark Tutorial - Spatial Joins\n",
    "\n",
    "---\n",
    "\n",
    "### Process\n",
    "1. Initialise a Spark session with Geospark enabled\n",
    "2. Load boundary and point datasets\n",
    "3. Convert them to geospatial DataFrames\n",
    "3. Perform a point in polygon spatial join\n",
    "\n",
    "---\n",
    "\n",
    "Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from pyspark.sql import SparkSession\n",
    "from geospark.register import upload_jars, GeoSparkRegistrator\n",
    "from geospark.utils import KryoSerializer, GeoSparkKryoRegistrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input path for parquet files\n",
    "input_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# number of processes to use (defaults to 2x physical CPUs)\n",
    "num_processors = cpu_count() * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Geospark's Java libraries to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_jars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 2.4.6 session initialised\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"query\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .config(\"spark.cores.max\", num_processors) \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark {} session initialised\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Geospark's User Defined Types (UDTs) and Functions (UDFs) with the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load boundary data from gzipped parquet files.\n",
    "\n",
    "Boundary geometries are polygons stored as OGC Well Known Text (WKT) strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- wkt_geom: string (nullable = true)\n",
      "\n",
      "+------+--------------------+\n",
      "|bdy_id|            wkt_geom|\n",
      "+------+--------------------+\n",
      "|  RA10|POLYGON((149.1082...|\n",
      "|  RA10|POLYGON((149.1914...|\n",
      "|  RA10|POLYGON((149.1914...|\n",
      "|  RA10|POLYGON((149.1914...|\n",
      "|  RA10|POLYGON((150.6666...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Loaded 17540 records\n"
     ]
    }
   ],
   "source": [
    "bdy_wkt_df = spark.read.parquet(os.path.join(input_path, \"boundaries\"))\n",
    "bdy_wkt_df.printSchema()\n",
    "bdy_wkt_df.show(5)\n",
    "\n",
    "print(\"Loaded {} records\".format(bdy_wkt_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view of the DataFrame to enable SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bdy_wkt_df.createOrReplaceTempView(\"bdy_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the point records from parquet files. The spatial data is stored in latitude & longitude (double precision) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+-----------+-------------------+------------------+\n",
      "|point_id   |latitude           |longitude         |\n",
      "+-----------+-------------------+------------------+\n",
      "|60000010000|-41.397873825618845|148.30298463001887|\n",
      "|60000020000|-40.91276600258191 |148.3205616825289 |\n",
      "|60000030000|-40.92217845040206 |148.32129834871355|\n",
      "|60000040000|-40.914773511737074|148.32338432564558|\n",
      "|60000050000|-40.91553126097137 |148.32378038710883|\n",
      "+-----------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Loaded 358009 records\n"
     ]
    }
   ],
   "source": [
    "point_wkt_df = spark.read.parquet(os.path.join(input_path, \"points\"))\n",
    "point_wkt_df.printSchema()\n",
    "point_wkt_df.show(5, False)\n",
    "\n",
    "print(\"Loaded {} records\".format(point_wkt_df.count()))\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_wkt_df.createOrReplaceTempView(\"point_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame of boundary IDs and geometries (i.e geospatial objects).\n",
    "\n",
    "They'll be spatially indexed automatically, enabling fast querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+------+--------------------+\n",
      "|bdy_id|                geom|\n",
      "+------+--------------------+\n",
      "|  RA10|POLYGON ((149.108...|\n",
      "|  RA10|POLYGON ((149.191...|\n",
      "|  RA10|POLYGON ((149.191...|\n",
      "|  RA10|POLYGON ((149.191...|\n",
      "|  RA10|POLYGON ((150.666...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdy_df = spark.sql(\"select bdy_id, st_geomFromWKT(wkt_geom) as geom from bdy_wkt\")\n",
    "bdy_df.printSchema()\n",
    "bdy_df.show(5)\n",
    "\n",
    "# create view to enable SQL queries\n",
    "bdy_df.createOrReplaceTempView(\"bdy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame of point IDs and geometries. Note the current limitation requiring Decimal lat/long fields as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+-----------+-----------------------------+\n",
      "|point_id   |geom                         |\n",
      "+-----------+-----------------------------+\n",
      "|60000010000|POINT (148.302985 -41.397874)|\n",
      "|60000020000|POINT (148.320562 -40.912766)|\n",
      "|60000030000|POINT (148.321298 -40.922178)|\n",
      "|60000040000|POINT (148.323384 -40.914774)|\n",
      "|60000050000|POINT (148.32378 -40.915531) |\n",
      "+-----------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"select point_id,\n",
    "                st_point(cast(longitude as decimal(9, 6)), cast(latitude as decimal(8, 6))) as geom\n",
    "         from point_wkt\"\"\"\n",
    "point_df = spark.sql(sql)\n",
    "point_df.printSchema()\n",
    "point_df.show(5, False)\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_df.createOrReplaceTempView(\"pnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a spatial join to boundary tag the points. Note it's an inner join, so point records could be lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+-----------+------+-----------------------------+\n",
      "|point_id   |bdy_id|geom                         |\n",
      "+-----------+------+-----------------------------+\n",
      "|90000200000|RA94  |POINT (96.888063 -12.197968) |\n",
      "|90000260000|RA94  |POINT (96.907247 -12.124957) |\n",
      "|90000101000|RA94  |POINT (105.634587 -10.488663)|\n",
      "|90000113000|RA94  |POINT (105.679505 -10.420269)|\n",
      "|90000102000|RA94  |POINT (105.672533 -10.437645)|\n",
      "+-----------+------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Boundary tagged 357720 points\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"SELECT pnt.point_id,\n",
    "                bdy.bdy_id,\n",
    "                pnt.geom\n",
    "         FROM pnt\n",
    "         INNER JOIN bdy ON ST_Intersects(pnt.geom, bdy.geom)\"\"\"\n",
    "join_df = spark.sql(sql)\n",
    "\n",
    "join_count = join_df.count()\n",
    "\n",
    "join_df.printSchema()\n",
    "join_df.show(5, False)\n",
    "\n",
    "print(\"Boundary tagged {} points\".format(join_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the Spark session and release its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}