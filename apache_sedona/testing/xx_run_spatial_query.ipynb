{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Apache Sedona Tutorial - Spatial Join\n",
    "\n",
    "---\n",
    "\n",
    "### Process\n",
    "1. Initialise a Spark session with Sedona enabled\n",
    "2. Load boundary and point datasets\n",
    "3. Convert them to geospatial DataFrames\n",
    "4. Perform a point in polygon spatial join\n",
    "5. Map the points, coloured by boundary ID\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import Python packages\n",
    "\n",
    "import os\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set input path for parquet files\n",
    "input_path = os.path.join(os.getcwd(), \"../data\")\n",
    "\n",
    "# set max number of processes (defaults to 2x physical CPUs)\n",
    "num_processors = cpu_count() * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.0.1 session initialised\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"query\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .config(\"spark.cores.max\", num_processors) \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark {} session initialised\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Sedona's User Defined Types (UDTs) and Functions (UDFs) with the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes\n",
    "\n",
    "#### 1. Load boundary data from gzipped parquet files\n",
    "\n",
    "Boundary geometries are polygons stored as OGC Well Known Text (WKT) strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- bdy_type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- wkt_geom: string (nullable = true)\n",
      "\n",
      "+------+--------------------+---------------+--------------------+\n",
      "|bdy_id|            bdy_type|          state|            wkt_geom|\n",
      "+------+--------------------+---------------+--------------------+\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON((149.1082...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON((149.1914...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON((149.1914...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON((149.1914...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON((149.2007...|\n",
      "+------+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Loaded 17540 records\n"
     ]
    }
   ],
   "source": [
    "bdy_wkt_df = spark.read.parquet(os.path.join(input_path, \"boundaries\"))\n",
    "bdy_wkt_df.printSchema()\n",
    "bdy_wkt_df.show(5)\n",
    "\n",
    "print(\"Loaded {} records\".format(bdy_wkt_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a view of the DataFrame to enable SQL queries\n",
    "bdy_wkt_df.createOrReplaceTempView(\"bdy_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load point data\n",
    "\n",
    "Spatial data is stored in latitude & longitude (double precision) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+-----------+---------------+-------------------+------------------+\n",
      "|point_id   |state          |latitude           |longitude         |\n",
      "+-----------+---------------+-------------------+------------------+\n",
      "|10000063000|New South Wales|-36.077980986803816|146.94716696685703|\n",
      "|10000152000|New South Wales|-36.04300542630207 |146.910526433595  |\n",
      "|10000200000|New South Wales|-36.05157619854574 |146.91949219941486|\n",
      "|10000310000|New South Wales|-36.070499212173615|146.8918609267459 |\n",
      "|10000393000|New South Wales|-35.939528641039175|147.01725670789446|\n",
      "+-----------+---------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Loaded 35800 records\n"
     ]
    }
   ],
   "source": [
    "point_wkt_df = spark.read.parquet(os.path.join(input_path, \"points\"))\n",
    "point_wkt_df.printSchema()\n",
    "point_wkt_df.show(5, False)\n",
    "\n",
    "print(\"Loaded {} records\".format(point_wkt_df.count()))\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_wkt_df.createOrReplaceTempView(\"point_wkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create geospatial dataframes\n",
    "\n",
    "#### 1. Create boundary geometries from WKT strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- bdy_type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+------+--------------------+---------------+--------------------+\n",
      "|bdy_id|            bdy_type|          state|                geom|\n",
      "+------+--------------------+---------------+--------------------+\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON ((149.108...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON ((149.191...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON ((149.191...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON ((149.191...|\n",
      "|  RA10|Major Cities of A...|New South Wales|POLYGON ((149.200...|\n",
      "+------+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdy_df = spark.sql(\"select bdy_id, bdy_type, state, st_geomFromWKT(wkt_geom) as geom from bdy_wkt\")\n",
    "#     .repartition(\"state\")\n",
    "bdy_df.printSchema()\n",
    "bdy_df.show(5)\n",
    "\n",
    "# create view to enable SQL queries\n",
    "bdy_df.createOrReplaceTempView(\"bdy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create point geometries. from lat/long fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+-----------+---------------+----------------------------------------------+\n",
      "|point_id   |state          |geom                                          |\n",
      "+-----------+---------------+----------------------------------------------+\n",
      "|10000063000|New South Wales|POINT (146.94716696685703 -36.077980986803816)|\n",
      "|10000152000|New South Wales|POINT (146.910526433595 -36.04300542630207)   |\n",
      "|10000200000|New South Wales|POINT (146.91949219941486 -36.05157619854574) |\n",
      "|10000310000|New South Wales|POINT (146.8918609267459 -36.070499212173615) |\n",
      "|10000393000|New South Wales|POINT (147.01725670789446 -35.939528641039175)|\n",
      "+-----------+---------------+----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_df = spark.sql(\"select point_id, state, st_point(longitude, latitude) as geom from point_wkt\")\n",
    "#     .repartition(\"state\")\n",
    "point_df.printSchema()\n",
    "point_df.show(5, False)\n",
    "\n",
    "# create view to enable SQL queries\n",
    "point_df.createOrReplaceTempView(\"pnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a spatial join to boundary tag the points\n",
    "\n",
    "##### Note:\n",
    "1. One of the dataframes will be spatially indexed automatically to speed up the query\n",
    "2. It's an inner join; point records could be lost in coastal areas or where there are gaps in the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_id: string (nullable = true)\n",
      " |-- bdy_id: string (nullable = true)\n",
      " |-- bdy_type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- geom: geometry (nullable = false)\n",
      "\n",
      "+-----------+------+------------------------+---------------+----------------------------------------------+\n",
      "|point_id   |bdy_id|bdy_type                |state          |geom                                          |\n",
      "+-----------+------+------------------------+---------------+----------------------------------------------+\n",
      "|10000063000|RA11  |Inner Regional Australia|New South Wales|POINT (146.94716696685703 -36.077980986803816)|\n",
      "|10000152000|RA11  |Inner Regional Australia|New South Wales|POINT (146.910526433595 -36.04300542630207)   |\n",
      "|10000200000|RA11  |Inner Regional Australia|New South Wales|POINT (146.91949219941486 -36.05157619854574) |\n",
      "|10000310000|RA11  |Inner Regional Australia|New South Wales|POINT (146.8918609267459 -36.070499212173615) |\n",
      "|10000393000|RA11  |Inner Regional Australia|New South Wales|POINT (147.01725670789446 -35.939528641039175)|\n",
      "+-----------+------+------------------------+---------------+----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Boundary tagged 35760 points\n",
      "Retrieved top 5 rows in 0:00:27.339580\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "sql = \"\"\"SELECT pnt.point_id,\n",
    "                bdy.bdy_id,\n",
    "                bdy.bdy_type,\n",
    "                bdy.state,\n",
    "                pnt.geom\n",
    "         FROM pnt\n",
    "         INNER JOIN bdy ON ST_Intersects(pnt.geom, bdy.geom)\"\"\"\n",
    "join_df = spark.sql(sql)\n",
    "\n",
    "join_count = join_df.count()\n",
    "\n",
    "join_df.printSchema()\n",
    "join_df.show(5, False)\n",
    "\n",
    "print(\"Boundary tagged {} points\".format(join_count))\n",
    "print(\"Retrieved top 5 rows in {}\".format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
